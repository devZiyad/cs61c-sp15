<html>
<head>
	<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1">
	<title>CS61C Spring 2015 Lab 10</title>
	<link rel="stylesheet" type="text/css" href="../style.css">
</head>

<body>
<div class="header">
	<h1>CS61C Spring 2015 Lab 10 - Thread Level Parallelism with OpenMP</h1>
</div>

<div class="content">
	<div class="section" id="goals">
		<h2><a name="goals">Goals</a></h2>
		<p>This semester, you have been developing on the new Dell Optiplex 9020 Workstations in 330 Soda. These are equipped with an Intel 3.4GHz i7 quad-core cpu with 32GB RAM (4x8GB) and 8 threads. Today, you will finally get a chance to take advantage of the multiple cores. 
		</p>
	</div>

	<div class="section" id="reading">
		<h2><a name="reading">Additional Reference</a></h2>
		<ul class="simple">
			<li><a href="http://openmp.org/mp-documents/omp-hands-on-SC08.pdf">

				Hands On Introduction to OpenMP</a></li>
			<li><a href="http://openmp.org/wp/resources/#Tutorials">
				Official OpenMP Tutorial Listing</a></li>
		</ul>
	</div>
	
	<div class="section" id="setup">
		<h2><a name="setup">Setup</a></h2>
		<p>Copy the contents of ~cs61c/labs/10 to a suitable location in
		your home directory.
		<pre class="output">
$ <span class='input'>cp -r ~cs61c/labs/10 ~/lab10</span></pre>
		</p>
		<b>Important:</b> If you are working remotely, make sure you are working on one of the Hive (hive{1-30}) computers.
	</div>
<br>
<div class="document" id="cs61c-lab-14">
	<h2><a name="intro">Introduction to OpenMP</a></h2>
	<div class="section" id="intro">
		<h3><a name="basics">Basics</a></h3>
		<p> OpenMP is a parallel programming framework for C/C++ and Fortran. It has gained quite
	a bit of traction in recent years, primarily due to its simplicity and
	good performance. In this lab we will be taking a quick peek at a small
	fraction of its features, but the links in the Additional Reference section
	can provide more information and tutorials for the interested.</p>

		<p>There are many types of parallelism and patterns for exploiting it.
    OpenMP chooses to use a nested fork-join model. By default, an OpenMP
    program is a normal sequential program, except for regions that the
    programmer explicitly declares to be executed in parallel. In the parallel
    region, the framework creates (forks) a set number of threads. Typically
    these threads all execute the same instructions, just on different
    portions of the data. At the end of the parallel region, the framework
    waits for all threads to complete (join) before it leaves that region and
    continues sequentially.</p>

		<center><img src="forkjoin.jpg"></center>

		<p>OpenMP uses <em>shared memory</em>, meaning all threads can access the
    same address space. The alternative to this is <em>distributed
    memory</em>, which is prevalent on clusters where data must be explicitly
    moved between address spaces. Many programmers find shared memory easier
    to program since they do not have to worry about moving their data, but it
    is usually harder to implement in hardware in a scalable way. Later in the
    lab we will declare some memory to be thread local (accessible only by the
    thread that created it) for performance reasons, but the programming
    framework provides the flexibility for threads to share memory without
    programmer effort.</p>
	</div>

	<div class="section" id="hello1">
		<h3><a name="hello1">Hello World Example</a></h3>
		<p>For this lab, we will use C to leverage our prior programming 
		experience with it. OpenMP is a framework with a C interface, and it is 
		not a built-in part of the language. Most OpenMP features are actually 
		directives to the compiler. Consider the following implementation of 
		Hello World (<code>hello.c</code>):
		<pre class="literal-block">
int main() {
  #pragma omp parallel
  {
    int thread_ID = omp_get_thread_num();
    printf(" hello world %d\n", thread_ID);
  }
}</pre>

		<p>This program will fork off the default number of threads and each
    thread will print out "hello world" in addition to which thread number it
    is. The <code>#pragma</code> tells the compiler that the rest of the line
    is a directive, and in this case it is <code>omp parallel</code>.
    <code>omp</code> declares that it is for OpenMP and <code>parallel</code>
    says the following code block (what is contained in { }) can be executed
    in parallel. Give it a try:</p>

		<pre class="output">
$ <span class='input'>make hello</span>
$ <span class='input'>./hello</span></pre>

		<p>Notice how the numbers are not necessarily in numerical order and not
    in the same order if you run <code>hello</code> multiple times. This is
    because within a <code>omp parallel</code> region, the programmer
    guarantees that the operations can be done in parallel, and there is no
    ordering between the threads. It is also worth noting that the variable
    <code>thread_ID</code> is <b>local</b> to each thread. In general with OpenMP,
    variables declared outside an <code>omp parallel</code> block have only one
    copy and are shared amongst all threads, while variables declared within a
    <code>omp parallel</code> block have a private copy for each thread.</p>

    
	</div>
</div>


<div class="section" id="exercises">
	<h2>Exercises</h2>
	<div class="section" id="exercise-1-v_add">
		<h3><a name="exercise-1-v_add">Exercise 1: Vector Addition</a></h3>
		<p>Vector addition is a naturally parallel computation, so it makes for a good 
		first exercise. The <code>v_add</code> function inside 
		<code>v_add.c</code> will return the array that is the cell-by-cell 
		addition of its inputs <code>x</code> and <code>y</code>. A first attempt 
		at this might look like:</p>

		
		<pre class="literal-block">
void v_add(double* x, double* y, double* z) {
  #pragma omp parallel
  {
    for(int i=0; i&#60;ARRAY_SIZE; i++)
      z[i] = x[i] + y[i];
  }
}</pre>

		<p>You can run this (<code>make v_add</code> followed by
    <code>./v_add</code>) and the testing framework will automatically time it
    and vary the number of threads. You will see that this actually seems to
    do worse as we increase the number of threads. The issue is that each
    thread is executing <b>all</b> of the code within the <code>omp parallel</code>
    block, meaning if we have 8 threads, we will actually be adding the
    vectors 8 times. To get speedup when increasing the number of threads, we
    need each thread to do less work, not the same amount as before.</p>

		<center><img src="decomp.jpg"></center>
		
		<p>Your task is to modify <code>v_add</code> so there is some speedup (speedup may plateau as the number of threads continues to increase). The best way to do this is to decrease the amount of work each thread does. 
To aid you in this process, two useful OpenMP functions are:</p>
	
		<pre class="literal-block">
int omp_get_num_threads();
int omp_get_thread_num();</pre>

		<p>The function <code>omp_get_num_threads()</code> will return how many
    threads there are in a <code>omp parallel</code> block, and
    <code>omp_get_thread_num()</code> will return the thread ID.</p>
<p>
Divide up the work for each thread through two different methods (write different code for each of these methods):
<ul>
	<li>First, have each thread handle adjacent sums: i.e. Thread 0 will add the 		elements at index 0, Thread 1 will add the elements at index 1, etc. This 	method will not be very efficient. It will encounter the problem known 	as <a
    href="http://en.wikipedia.org/wiki/False_sharing">false sharing</a>. </li>
	<li>Second, if there are N threads, break the vectors into N contiguous chunks, and have each thread only add that chunk (like the figure above).</li>
</ul>
</p>

		<p>For this exercise, we are asking you to manually split the work amongst
    threads. Since this is such a common pattern for software, the designers
    of OpenMP made the <code>for</code> directive to automatically split up
    independent work. Here is the function rewritten using it. You may NOT use
    this directive in your solution to this exercise (Exercise 1).</p>
		
		<pre class="literal-block">
void v_add(double* x, double* y, double* z) {
  #pragma omp parallel
  {
    #pragma omp for
    for(int i=0; i&#60;ARRAY_SIZE; i++)
      z[i] = x[i] + y[i];
  }
}</pre>
		
		<div class="section" id="ex1">
			<h4><a name="id1">Checkoff</a></h4>
			<ul>
				<li>Show the TA your code for <code>v_add</code> that manually splits up 
					the work (both methods) </li>
				<li>Run your code to show that it gets parallel speedup (second method)</li>
			</ul>
		</div>
	</div>

  	<div class="section" id="exercise-2-dotp">
		<h3><a name="exercise-2-dotp">Exercise 2: Dot Product</a></h3>
		<p>The next interesting computation we want to compute is the <a
    href="http://en.wikipedia.org/wiki/Dot_product">dot product</a> of two
    vectors. At first glance, implementing this might seem not too dissimilar
    from <code>v_add</code>, but the challenge is how to sum up all of the
    products into the same variable (reduction). A sloppy handling of reduction
	may lead to data races: all the threads are trying to read and write to
	 the same address simultaneously. One solution is to use a <b>critical section</b>. The code in
	a critical section can only be executed by a single thread at any given time. 
	Thus, having a critical section naturally prevents multiple threads from
	reading and writing to the same data, a problem that would otherwise lead
	 to data races. A naive implementation would
    protect the sum with a critical section, like 
    (<code>dotp.c</code>):</p>

	
		<pre class="literal-block">
double dotp(double* x, double* y) {
  double global_sum = 0.0;
  #pragma omp parallel
  {
    #pragma omp for
    for(int i=0; i&#60;ARRAY_SIZE; i++)
      #pragma omp critical
        global_sum += x[i] * y[i];
  }
  return global_sum;
}</pre>

		<p>Try out the code (<code>make dotp</code> and <code>./dotp</code>).
    Notice how the performance gets <em>much</em> worse as the number of
    threads goes up? By putting all of the work of reduction in a critical section, we have 
    flattened the parallelism and made it so only one thread can do useful work 
    at a time (not exactly the idea behind thread-level parallelism). 
	This <em>contention</em> is problematic; each thread is constantly fighting
	for the critical section and only one is making any progress at any given time. As the number of
    threads goes up, so does the contention, and the performance pays the
    price. Can you fix this performance bottleneck?</p>

		<p>First, try fixing this code without using the OpenMP Reduction 
    keyword. Hint: Try reducing the number of times a thread must
    add to the shared <code>global_sum</code> variable.</p>
    
    <p>Now, fix the problem using OpenMP's built-in Reduction keyword (Google for more information on it).
    Is your performance any better than in the case of the manual fix? Why?</p>

		<div class="section" id="ex2">
			<h4><a name="id2">Checkoff</a></h4>
			<ul>
			<li>Show the TA your manual fix to <code>dotp.c</code> that gets speedup 
            over the single threaded case</li>
        	<li>Show the TA your Reduction keyword fix for <code>dotp.c</code>,
        	and explain the difference in performance.</li>
        	</ul>
		</div>
	</div>

	
</div>
</div>
</body>
</html>
